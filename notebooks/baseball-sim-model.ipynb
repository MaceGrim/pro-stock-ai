{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_900969/677716641.py:4: DtypeWarning: Columns (43) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# file_path = '../data/statcast-short.csv'\n",
    "file_path = '../data/statcast_data_2016_2023.csv'\n",
    "data = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "game_date\n",
       "2016    716073\n",
       "2017    721244\n",
       "2018    721190\n",
       "2019    749399\n",
       "2020    263584\n",
       "2021    709852\n",
       "2022    712392\n",
       "2023    717945\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"game_date\"] = pd.to_datetime(data[\"game_date\"])\n",
    "\n",
    "# How many pitches per year?\n",
    "data[\"game_date\"].dt.year.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_900969/2909203750.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['events'].fillna(data[\"description\"], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "events\n",
       "ball                   1677532\n",
       "foul                    932675\n",
       "called_strike           810557\n",
       "swinging_strike         356935\n",
       "strikeout               304841\n",
       "                        ...   \n",
       "field_out 3.0 9.0            1\n",
       "double 9.0 6.0               1\n",
       "double_play 9.0 5.0          1\n",
       "field_out 8.0 0.0            1\n",
       "triple 4.0 5.0               1\n",
       "Name: count, Length: 757, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess data\n",
    "data['events'].fillna(data[\"description\"], inplace=True)\n",
    "\n",
    "# Encode position within the event. Shown to be useful in the {batter, pitcher}2vec paper. There's probably a better way, but this works for now.\n",
    "# data[\"events\"] = data[\"events\"] + \" \" + data[\"hit_location\"].astype(str)\n",
    "\n",
    "# Bin the hc_x and hc_y columns\n",
    "data[\"hc_x_bin\"] = pd.cut(data[\"hc_x\"], bins=10, labels=False)\n",
    "data[\"hc_y_bin\"] = pd.cut(data[\"hc_y\"], bins=10, labels=False)\n",
    "\n",
    "location_string = data[\"hc_x_bin\"].astype(str) + \" \" + data[\"hc_y_bin\"].astype(str)\n",
    "\n",
    "data[\"events\"] = (data[\"events\"] + \" \" + location_string).str.replace(\"nan\", \"\").str.strip()\n",
    "\n",
    "data[\"events\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode events\n",
    "\n",
    "event_to_idx = {event: idx for idx, event in enumerate(data['events'].unique())}\n",
    "idx_to_event = {idx: event for event, idx in event_to_idx.items()}\n",
    "data['event_idx'] = data['events'].map(event_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "events\n",
       "ball                   1677532\n",
       "foul                    932675\n",
       "called_strike           810557\n",
       "swinging_strike         356935\n",
       "strikeout               304841\n",
       "                        ...   \n",
       "field_out 3.0 9.0            1\n",
       "double 9.0 6.0               1\n",
       "double_play 9.0 5.0          1\n",
       "field_out 8.0 0.0            1\n",
       "triple 4.0 5.0               1\n",
       "Name: count, Length: 757, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"events\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine batters and pitchers to get all players\n",
    "all_players = pd.concat([data['batter'], data['pitcher']]).unique()\n",
    "\n",
    "# Create a LabelEncoder for all players\n",
    "le_players = LabelEncoder()\n",
    "le_players.fit(all_players)\n",
    "\n",
    "# Transform batter and pitcher columns\n",
    "data['batter'] = le_players.transform(data['batter'])\n",
    "data['pitcher'] = le_players.transform(data['pitcher'])\n",
    "\n",
    "# Get the number of unique players\n",
    "num_players = len(le_players.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique pitch types: 20\n"
     ]
    }
   ],
   "source": [
    "# Create a LabelEncoder for pitch types\n",
    "le_pitch_types = LabelEncoder()\n",
    "le_pitch_types.fit(data['pitch_type'])\n",
    "\n",
    "# Transform pitch_type column\n",
    "data['pitch_type'] = le_pitch_types.transform(data['pitch_type'])\n",
    "\n",
    "# Get the number of unique pitch types\n",
    "num_pitch_types = len(le_pitch_types.classes_)\n",
    "print(f\"Number of unique pitch types: {num_pitch_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = data.iloc[int(0.9*len(data)):]\n",
    "data = data.iloc[:int(0.9*len(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "class BaseballDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        batter = row['batter']\n",
    "        pitcher = row['pitcher']\n",
    "        pitch_type = row['pitch_type']\n",
    "        release_speed = row['release_speed']\n",
    "        release_pos_x = row['release_pos_x']\n",
    "        release_pos_z = row['release_pos_z']\n",
    "        game_state = row[['home_score', 'away_score', 'balls', 'strikes', 'outs_when_up', 'inning']].values.astype(np.float32)\n",
    "        outcome = row['event_idx']\n",
    "\n",
    "        return torch.tensor([batter, pitcher], dtype=torch.long), \\\n",
    "               torch.tensor([pitch_type, release_speed, release_pos_x, release_pos_z], dtype=torch.float32), \\\n",
    "               torch.tensor(game_state, dtype=torch.float32), \\\n",
    "               torch.tensor(outcome, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BaseballModel(nn.Module):\n",
    "#     def __init__(self, num_players, embedding_dim, game_state_dim, hidden_dim, output_dim):\n",
    "#         super(BaseballModel, self).__init__()\n",
    "#         self.batter_embedding = nn.Embedding(num_players, embedding_dim)\n",
    "#         self.pitcher_embedding = nn.Embedding(num_players, embedding_dim)\n",
    "#         self.pitch_model = nn.Sequential(\n",
    "#             nn.Linear(embedding_dim * 2 + game_state_dim, hidden_dim),  # 50 + 50 + 6 = 106\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, 4)  # Predict pitch_type, release_speed, release_pos_x, release_pos_z\n",
    "#         )\n",
    "#         self.outcome_model = nn.Sequential(\n",
    "#             nn.Linear(embedding_dim * 2 + 4 + game_state_dim, hidden_dim),  # 50 + 50 + 4 + 6 = 110\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, output_dim)  # Predict outcome\n",
    "#         )\n",
    "\n",
    "#     def forward(self, players, game_state):\n",
    "#         batter_emb = self.batter_embedding(players[:, 0])\n",
    "#         pitcher_emb = self.pitcher_embedding(players[:, 1])\n",
    "#         x = torch.cat([batter_emb, pitcher_emb, game_state], dim=1)\n",
    "\n",
    "#         pitch_pred = self.pitch_model(x)\n",
    "\n",
    "#         outcome_input = torch.cat([batter_emb, pitcher_emb, pitch_pred, game_state], dim=1)\n",
    "#         outcome_pred = self.outcome_model(outcome_input)\n",
    "\n",
    "#         return pitch_pred, outcome_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseballModel(nn.Module):\n",
    "    def __init__(self, num_players, embedding_dim, game_state_dim, hidden_dim, output_dim):\n",
    "        super(BaseballModel, self).__init__()\n",
    "        self.batter_embedding = nn.Embedding(num_players, embedding_dim)\n",
    "        self.pitcher_embedding = nn.Embedding(num_players, embedding_dim)\n",
    "        \n",
    "        self.pitch_model = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2 + game_state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, 4)  # Predict pitch_type, release_speed, release_pos_x, release_pos_z\n",
    "        )\n",
    "        \n",
    "        self.outcome_model = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * 2 + 4 + game_state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, output_dim)  # Predict outcome\n",
    "        )\n",
    "\n",
    "    def forward(self, players, game_state):\n",
    "        batter_emb = self.batter_embedding(players[:, 0])\n",
    "        pitcher_emb = self.pitcher_embedding(players[:, 1])\n",
    "        x = torch.cat([batter_emb, pitcher_emb, game_state], dim=1)\n",
    "\n",
    "        pitch_pred = self.pitch_model(x)\n",
    "\n",
    "        outcome_input = torch.cat([batter_emb, pitcher_emb, pitch_pred, game_state], dim=1)\n",
    "        outcome_pred = self.outcome_model(outcome_input)\n",
    "\n",
    "        return pitch_pred, outcome_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "num_players = data['batter'].nunique() + data['pitcher'].nunique()\n",
    "embedding_dim = 128\n",
    "game_state_dim = 6\n",
    "hidden_dim = 256\n",
    "output_dim = len(event_to_idx)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "model = BaseballModel(num_players, embedding_dim, game_state_dim, hidden_dim, output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, avg_loss, avg_val_loss, filename):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'avg_loss': avg_loss,\n",
    "        'avg_val_loss': avg_val_loss,\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "def load_checkpoint(filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    avg_loss = checkpoint['avg_loss']\n",
    "    avg_val_loss = checkpoint['avg_val_loss']\n",
    "    return epoch, avg_loss, avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_900969/439902254.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from ../notebooks/checkpoints/checkpoint_epoch_50_1722738319.pth, starting at epoch 50, loss: 2.5178525924682615, val loss: 2.4092178344726562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Use this when you've found the right batch size\n",
    "\n",
    "train_dataset = BaseballDataset(data)\n",
    "val_dataset = BaseballDataset(val_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1048576, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1048576, shuffle=False)\n",
    "\n",
    "num_epochs = 100\n",
    "checkpoint_dir = 'checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "avg_losses = []\n",
    "avg_val_losses = []\n",
    "\n",
    "load_checkpoint_filename = \"../notebooks/checkpoints/checkpoint_epoch_50_1722738319.pth\"\n",
    "if load_checkpoint_filename:\n",
    "    epoch, avg_loss, avg_val_loss = load_checkpoint(load_checkpoint_filename)\n",
    "    print(f\"Loaded checkpoint from {load_checkpoint_filename}, starting at epoch {epoch}, loss: {avg_loss}, val loss: {avg_val_loss}\")\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(epoch + 1, epoch + num_epochs + 1)):\n",
    "    epoch_start_time = time.time()\n",
    "    total_loss = 0\n",
    "    total_val_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    for players, pitch_characteristics, game_state, outcome in train_loader:\n",
    "        # Move data to GPU\n",
    "        players = players.to(device)\n",
    "        pitch_characteristics = pitch_characteristics.to(device)\n",
    "        game_state = game_state.to(device)\n",
    "        outcome = outcome.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pitch_pred, outcome_pred = model(players, game_state)\n",
    "\n",
    "        loss = criterion(outcome_pred, outcome)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for players, pitch_characteristics, game_state, outcome in val_loader:\n",
    "            players = players.to(device)\n",
    "            pitch_characteristics = pitch_characteristics.to(device)\n",
    "            game_state = game_state.to(device)\n",
    "            outcome = outcome.to(device)\n",
    "\n",
    "            pitch_pred, outcome_pred = model(players, game_state)\n",
    "\n",
    "            val_loss = criterion(outcome_pred, outcome)\n",
    "            total_val_loss += val_loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    avg_losses.append(avg_loss)\n",
    "    avg_val_losses.append(avg_val_loss)\n",
    "\n",
    "    epoch_end_time = time.time()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Data Per Second: {len(data) / (epoch_end_time - epoch_start_time):.2f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint_filename = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}_{int(time.time())}.pth')\n",
    "    save_checkpoint(model, optimizer, epoch+1, avg_loss, avg_val_loss, checkpoint_filename)\n",
    "\n",
    "print(\"Training complete\")\n",
    "\n",
    "# After training, if you want to use the model on CPU again\n",
    "model = model.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:   0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:   6%|▌         | 1/17 [00:00<00:12,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32, Loss: 6.8497, Val Loss: 6.5881, Data/s: 199.42, Est. Epoch Time: 23972.46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:  12%|█▏        | 2/17 [00:01<00:11,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64, Loss: 6.8335, Val Loss: 6.5891, Data/s: 436.45, Est. Epoch Time: 10953.18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:  18%|█▊        | 3/17 [00:02<00:12,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 128, Loss: 6.7736, Val Loss: 6.5848, Data/s: 583.93, Est. Epoch Time: 8186.81s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:  24%|██▎       | 4/17 [00:04<00:16,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 256, Loss: 6.7374, Val Loss: 6.5752, Data/s: 675.54, Est. Epoch Time: 7076.55s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:  29%|██▉       | 5/17 [00:07<00:23,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 512, Loss: 6.7014, Val Loss: 6.5642, Data/s: 810.08, Est. Epoch Time: 5901.25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:  35%|███▌      | 6/17 [00:13<00:35,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1024, Loss: 6.6296, Val Loss: 6.5304, Data/s: 887.45, Est. Epoch Time: 5386.78s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:  41%|████      | 7/17 [00:24<00:58,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 2048, Loss: 6.6074, Val Loss: 6.5169, Data/s: 917.15, Est. Epoch Time: 5212.33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:  47%|████▋     | 8/17 [00:46<01:38, 10.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 4096, Loss: 6.5425, Val Loss: 6.4826, Data/s: 939.21, Est. Epoch Time: 5089.91s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:  53%|█████▎    | 9/17 [01:30<02:49, 21.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 8192, Loss: 6.4895, Val Loss: 6.4416, Data/s: 936.72, Est. Epoch Time: 5103.47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:  59%|█████▉    | 10/17 [02:59<04:54, 42.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16384, Loss: 6.4386, Val Loss: 6.3941, Data/s: 921.96, Est. Epoch Time: 5185.16s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:  65%|██████▍   | 11/17 [05:54<08:17, 82.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 32768, Loss: 6.3750, Val Loss: 6.3390, Data/s: 934.16, Est. Epoch Time: 5117.47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:  71%|███████   | 12/17 [11:44<13:40, 164.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 65536, Loss: 6.3007, Val Loss: 6.2733, Data/s: 936.76, Est. Epoch Time: 5103.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:  76%|███████▋  | 13/17 [21:36<19:35, 293.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 131072, Loss: 6.2136, Val Loss: 6.1925, Data/s: 1106.21, Est. Epoch Time: 4321.53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:  82%|████████▏ | 14/17 [37:31<24:40, 493.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 262144, Loss: 6.1090, Val Loss: 3.6515, Data/s: 1372.57, Est. Epoch Time: 3482.88s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:  88%|████████▊ | 15/17 [1:05:40<28:27, 853.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 524288, Loss: 5.9824, Val Loss: 2.3779, Data/s: 1552.52, Est. Epoch Time: 3079.20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing batch sizes:  94%|█████████▍| 16/17 [1:46:14<22:09, 1329.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 1048576, Loss: 5.8300, Val Loss: 1.1522, Data/s: 2153.92, Est. Epoch Time: 2219.45s\n"
     ]
    }
   ],
   "source": [
    "# # Use this to find the right batch size for your machine/params for SPEED\n",
    "\n",
    "# import time\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def train_and_evaluate_sample(model, train_dataset, val_dataset, batch_size, num_batches, device, criterion, optimizer):\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     total_val_loss = 0\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # Train on a sample of batches\n",
    "#     for i, (players, pitch_characteristics, game_state, outcome) in enumerate(train_loader):\n",
    "#         if i >= num_batches:\n",
    "#             break\n",
    "\n",
    "#         players = players.to(device)\n",
    "#         pitch_characteristics = pitch_characteristics.to(device)\n",
    "#         game_state = game_state.to(device)\n",
    "#         outcome = outcome.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         pitch_pred, outcome_pred = model(players, game_state)\n",
    "#         loss = criterion(outcome_pred, outcome)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     # Validate on a sample of batches\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for i, (players, pitch_characteristics, game_state, outcome) in enumerate(val_loader):\n",
    "#             if i >= num_batches:\n",
    "#                 break\n",
    "\n",
    "#             players = players.to(device)\n",
    "#             pitch_characteristics = pitch_characteristics.to(device)\n",
    "#             game_state = game_state.to(device)\n",
    "#             outcome = outcome.to(device)\n",
    "\n",
    "#             pitch_pred, outcome_pred = model(players, game_state)\n",
    "#             val_loss = criterion(outcome_pred, outcome)\n",
    "#             total_val_loss += val_loss.item()\n",
    "\n",
    "#     avg_loss = total_loss / num_batches\n",
    "#     avg_val_loss = total_val_loss / num_batches\n",
    "#     sample_runtime = time.time() - start_time\n",
    "\n",
    "#     # Estimate full epoch time\n",
    "#     estimated_epoch_time = sample_runtime * (len(train_dataset) / (batch_size * num_batches))\n",
    "\n",
    "#     return avg_loss, avg_val_loss, estimated_epoch_time\n",
    "\n",
    "# def find_optimal_batch_size(model, train_dataset, val_dataset, device, criterion, optimizer):\n",
    "#     batch_sizes = [32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576, 2097152]\n",
    "#     num_batches = 5  # Number of batches to sample for each batch size\n",
    "#     results = []\n",
    "\n",
    "#     for batch_size in tqdm(batch_sizes, desc=\"Testing batch sizes\"):\n",
    "#         model.to(device)\n",
    "#         avg_loss, avg_val_loss, estimated_epoch_time = train_and_evaluate_sample(\n",
    "#             model, train_dataset, val_dataset, batch_size, num_batches, device, criterion, optimizer\n",
    "#         )\n",
    "#         data_per_second = len(train_dataset) / estimated_epoch_time\n",
    "#         results.append({\n",
    "#             'batch_size': batch_size,\n",
    "#             'avg_loss': avg_loss,\n",
    "#             'avg_val_loss': avg_val_loss,\n",
    "#             'data_per_second': data_per_second,\n",
    "#             'estimated_epoch_time': estimated_epoch_time\n",
    "#         })\n",
    "#         print(f\"Batch size: {batch_size}, Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Data/s: {data_per_second:.2f}, Est. Epoch Time: {estimated_epoch_time:.2f}s\")\n",
    "\n",
    "#     # Find the batch size with the lowest validation loss\n",
    "#     best_batch_size = min(results, key=lambda x: x['avg_val_loss'])['batch_size']\n",
    "    \n",
    "#     print(\"\\nResults:\")\n",
    "#     for result in results:\n",
    "#         print(f\"Batch size: {result['batch_size']}, Loss: {result['avg_loss']:.4f}, Val Loss: {result['avg_val_loss']:.4f}, Data/s: {result['data_per_second']:.2f}, Est. Epoch Time: {result['estimated_epoch_time']:.2f}s\")\n",
    "    \n",
    "#     print(f\"\\nBest batch size based on validation loss: {best_batch_size}\")\n",
    "\n",
    "#     # Save results to CSV\n",
    "#     df = pd.DataFrame(results)\n",
    "#     df.to_csv('batch_size_results.csv', index=False)\n",
    "#     print(\"Results saved to batch_size_results.csv\")\n",
    "\n",
    "#     return results, best_batch_size\n",
    "\n",
    "# train_dataset = BaseballDataset(data)\n",
    "# val_dataset = BaseballDataset(val_data)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=262144, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=262144, shuffle=False)\n",
    "\n",
    "# # Usage\n",
    "# results, best_batch_size = find_optimal_batch_size(model, train_dataset, val_dataset, device, criterion, optimizer)\n",
    "\n",
    "# # Train the model with the best batch size\n",
    "# best_train_loader = DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True)\n",
    "# best_val_loader = DataLoader(val_dataset, batch_size=best_batch_size, shuffle=False)\n",
    "\n",
    "# # ... (rest of your training code using best_train_loader and best_val_loader)\n",
    "\n",
    "# print(\"Training complete\")\n",
    "\n",
    "# # After training, if you want to use the model on CPU again\n",
    "# model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot the losses\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mavg_losses\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(avg_val_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'avg_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(avg_losses, label='Training Loss')\n",
    "plt.plot(avg_val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'enhanced_baseball_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, player_data, game_state_data, sample=False):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        players = torch.tensor(player_data, dtype=torch.long).unsqueeze(0)\n",
    "        game_state = torch.tensor(game_state_data, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        pitch_pred, outcome_pred = model(players, game_state)\n",
    "        \n",
    "        outcome_prob = torch.softmax(outcome_pred, dim=1)\n",
    "        \n",
    "        if sample:\n",
    "            # Sample from the probability distribution\n",
    "            predicted_outcome = torch.multinomial(outcome_prob, num_samples=1).item()\n",
    "        else:\n",
    "            # Choose the most likely outcome (argmax)\n",
    "            predicted_outcome = torch.argmax(outcome_prob, dim=1).item()\n",
    "        \n",
    "        return idx_to_event[predicted_outcome]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1327, 743]\n",
      "Predicted event: other_out\n"
     ]
    }
   ],
   "source": [
    "# Example prediction\n",
    "model = model.to(\"cpu\")\n",
    "player_data = [data.iloc[5]['batter'], data.iloc[5]['pitcher']]\n",
    "game_state_data = data.iloc[0][['home_score', 'away_score', 'balls', 'strikes', 'outs_when_up', 'inning']].values.astype(np.float32)\n",
    "\n",
    "print(player_data)\n",
    "\n",
    "predicted_event = predict(model, player_data, game_state_data, sample=True)\n",
    "print(f'Predicted event: {predicted_event}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claymodel-main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
